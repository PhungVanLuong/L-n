{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ƒê·ªçc file CSV g·ªëc\n",
        "df = pd.read_csv(\"train_data.csv\")\n",
        "df[\"Related_Knowledge\"] = df[\"problem\"].apply(lambda p: retrieve_context(p, k=3))\n",
        "df.to_csv(\"train_with_knowledge.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "FF1n4mLpVZGa",
        "outputId": "fb87af4e-d634-44ec-f6f3-cf3906556934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lTDDP4YscavI",
        "outputId": "0731b4c2-37e4-4ed6-fdd2-858f5005c750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "5WXSxc72kHEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ziye2chen/LLMs-for-Mathematical-Analysis.git"
      ],
      "metadata": {
        "id": "RlY1LX02kRto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install faiss-cpu sentence-transformers\n",
        "!pip install unsloth transformers trl datasets pandas matplotlib rich scipy sentencepiece"
      ],
      "metadata": {
        "id": "lA8YuRTwkS8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZOLpBveDj9Wv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fFLO6BCzj9W1"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EN8y6T-bj9W1",
        "outputId": "3c3e947e-d630-4b28-9425-ced746c92f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch versionÔºö\n",
            "2.7.1+cu126\n",
            "CUDA Version: \n",
            "12.6\n",
            "cuDNN version is :\n",
            "90501\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Pytorch versionÔºö\")\n",
        "print(torch.__version__)\n",
        "print(\"CUDA Version: \")\n",
        "print(torch.version.cuda)\n",
        "print(\"cuDNN version is :\")\n",
        "print(torch.backends.cudnn.version())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# ƒê·ªçc file train d·ªØ li·ªáu ƒë·ªÉ d√πng cho truy h·ªìi\n",
        "df = pd.read_csv(\"/content/train_data.csv\")\n",
        "\n",
        "# G·ªôp problem + solution th√†nh vƒÉn b·∫£n corpus v√† th√™m prefix \"passage: \"\n",
        "texts = (\"passage: \" + df['problem'] + \"\\n\\nL·ªùi gi·∫£i:\\n\" + df['solution']).tolist()\n",
        "\n",
        "# L∆∞u corpus\n",
        "with open(\"rag_texts.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# T·∫°o model embedding\n",
        "embedder = SentenceTransformer('intfloat/multilingual-e5-base')\n",
        "embeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# T·∫°o FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# L∆∞u index\n",
        "faiss.write_index(index, \"rag_index.faiss\")\n"
      ],
      "metadata": {
        "id": "m5ccecwu3_xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaiEj8bkj9W5"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 10240 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-Math-7B-bnb-4bit\", # \"unsloth/Meta-Llama-3.1-8B\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLHuhv-Gj9W7"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPL2DK3Bj9W8"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqa2l2Vej9W9"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"L√† tr·ª£ l√Ω to√°n h·ªçc, b·∫°n c·∫ßn ph√¢n t√≠ch b√†i to√°n ƒë·ªÉ t√¨m ra lo·∫°i b√†i to√°n ƒë√≥ trong Ph√¢n t√≠ch Th·ª±c t·∫ø. Cung c·∫•p Problem_Type c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£i b√†i to√°n n√†y.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "{}\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tEZPMCvj9W-"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    Problem_Type = examples[\"problem_type\"]\n",
        "    Problem = examples[\"problem\"]\n",
        "\n",
        "    texts = []\n",
        "    for problem, problem_type in zip(Problem, Problem_Type):\n",
        "        text = alpaca_prompt.format(problem, problem_type) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv',data_files = '/content/train_data.csv', split='train')\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Llz9XXpj9W_"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset, # dataset -> tokenized_train_dataset\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 2,\n",
        "\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 300, # 60 -> 10\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6ktQpOrj9XA"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7rC9fQPj9XA"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1s3cZoxj9XB"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"MathAnalysis_Qwen_Classifier\") # Local saving\n",
        "tokenizer.save_pretrained(\"MathAnalysis_Qwen_Classifier\")\n",
        "model.save_pretrained(\"/content/drive/MyDrive/MathAnalysis_Qwen_Classifier\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/MathAnalysis_Qwen_Classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpkOb6jQj9XB"
      },
      "outputs": [],
      "source": [
        "test_prompt = \"\"\"L√† tr·ª£ l√Ω to√°n h·ªçc, b·∫°n c·∫ßn ph√¢n t√≠ch b√†i to√°n ƒë·ªÉ t√¨m ra lo·∫°i b√†i to√°n ƒë√≥ trong Ph√¢n t√≠ch Th·ª±c t·∫ø. Cung c·∫•p Problem_Type c√≥ th·ªÉ ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ gi·∫£i b√†i to√°n n√†y.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGwYqzL4j9XC"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 10240 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"MathAnalysis_Qwen_Classifier\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# # alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "# inputs = tokenizer(\n",
        "# [\n",
        "#     test_prompt.format(\n",
        "#         \"Cho ba s·ªë d∆∞∆°ng \\( a, b, c \\) tho·∫£ m√£n \\( b \\ne c \\), \\( \\sqrt{a} + \\sqrt{b} \\ne \\sqrt{c} \\), \\( a + b = (\\sqrt{a} + \\sqrt{b} - \\sqrt{c})^2 \\). Ch·ª©ng minh ƒë·∫≥ng th·ª©c: \\[ \\frac{a + (\\sqrt{a} - \\sqrt{c})^2}{b + (\\sqrt{b} - \\sqrt{c})^2} = \\frac{\\sqrt{a} - \\sqrt{c}}{\\sqrt{b} - \\sqrt{c}}. \\]\", # Problem\n",
        "#     )\n",
        "# ], return_tensors = \"pt\").to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "texts = json.load(open(\"rag_texts.json\", \"r\", encoding=\"utf-8\"))\n",
        "index = faiss.read_index(\"rag_index.faiss\")\n",
        "embedder = SentenceTransformer('intfloat/multilingual-e5-base')\n",
        "\n",
        "def retrieve_context(query, k=3):\n",
        "    query_embedding = embedder.encode([\"query: \" + query])\n",
        "    D, I = index.search(query_embedding, k)\n",
        "    return [texts[i] for i in I[0]]\n",
        "\n",
        "# STEP 3: Prompt + Truy xu·∫•t + Sinh l·ªùi gi·∫£i\n",
        "raw_problem = \"Cho ba s·ªë d∆∞∆°ng \\( a, b, c \\) tho·∫£ m√£n \\( b \\ne c \\), \\( \\sqrt{a} + \\sqrt{b} \\ne \\sqrt{c} \\), \\( a + b = (\\sqrt{a} + \\sqrt{b} - \\sqrt{c})^2 \\). Ch·ª©ng minh ƒë·∫≥ng th·ª©c: \\[ \\frac{a + (\\sqrt{a} - \\sqrt{c})^2}{b + (\\sqrt{b} - \\sqrt{c})^2} = \\frac{\\sqrt{a} - \\sqrt{c}}{\\sqrt{b} - \\sqrt{c}}. \\]\"\n",
        "contexts = retrieve_context(raw_problem, k=3)\n",
        "context_block = \"\\n\\n\".join(contexts)\n",
        "\n",
        "final_prompt = context_block + \"\\n\\n\" + test_prompt.format(raw_problem)\n"
      ],
      "metadata": {
        "id": "L5XtmXxv57Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5iOdfsqj9XD"
      },
      "outputs": [],
      "source": [
        "# # T·ª± t√≠nh max_new_tokens an to√†n\n",
        "# input_len = inputs.input_ids.shape[-1]\n",
        "# max_total_length = max_seq_length  # v√≠ d·ª•: 10240\n",
        "# safe_max_new_tokens = max_total_length - input_len\n",
        "\n",
        "# Tokenize & sinh\n",
        "inputs = tokenizer([final_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "input_len = inputs.input_ids.shape[-1]\n",
        "safe_max_new_tokens = max_seq_length - input_len\n",
        "\n",
        "# In th·ª≠ ki·ªÉm tra\n",
        "print(f\"Input tokens: {input_len}, Max new tokens: {safe_max_new_tokens}\")\n",
        "\n",
        "# D√πng TextStreamer ƒë·ªÉ in ra t·ª´ng d√≤ng sinh token\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Sinh k·∫øt qu·∫£\n",
        "_ = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=safe_max_new_tokens,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import numpy as np\n",
        "\n",
        "test_prompt_rag = \"\"\"B·∫°n l√† tr·ª£ l√Ω to√°n h·ªçc. D∆∞·ªõi ƒë√¢y l√† m·ªôt b√†i to√°n h·ªçc c·∫ßn ph√¢n lo·∫°i Problem_Type.\n",
        "B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng c√°c v√≠ d·ª• t∆∞∆°ng t·ª± t·ª´ d·ªØ li·ªáu d∆∞·ªõi ƒë√¢y ƒë·ªÉ h·ªó tr·ª£.\n",
        "\n",
        "### C√°c v√≠ d·ª• t∆∞∆°ng t·ª±:\n",
        "{}\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Load m√¥ h√¨nh ƒë√£ fine-tune\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#         model_name = \"/content/drive/MyDrive/MathAnalysis_Qwen_Classifier\",\n",
        "#         max_seq_length = 4096,\n",
        "#         dtype = torch.float16,\n",
        "#         load_in_4bit = True,\n",
        "# )\n",
        "# FastLanguageModel.for_inference(model)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Load l·∫°i FAISS index v√† embedding model\n",
        "kb_df = pd.read_csv(\"train_data.csv\")\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "faiss_index = faiss.read_index(\"rag_index.faiss\")\n",
        "\n",
        "df = pd.read_csv(\"test_data.csv\")\n",
        "\n",
        "outputs = []\n",
        "for idx, row in df.iterrows():\n",
        "    problem = row[\"problem\"]\n",
        "\n",
        "    # Embed problem\n",
        "    problem_emb = embedding_model.encode([problem])\n",
        "    top_k = 3\n",
        "    distances, indices = faiss_index.search(np.array(problem_emb), top_k)\n",
        "\n",
        "    # Tr√≠ch xu·∫•t context\n",
        "    retrieved_examples = []\n",
        "    for i in indices[0]:\n",
        "        example = f\"- Problem: {kb_df.iloc[i]['problem']}\\n  Problem_Type: {kb_df.iloc[i]['problem_type']}\"\n",
        "        retrieved_examples.append(example)\n",
        "    rag_context = \"\\n\".join(retrieved_examples)\n",
        "\n",
        "    # Format prompt v·ªõi context\n",
        "    prompt = test_prompt_rag.format(rag_context, problem)\n",
        "\n",
        "    # Tokenize v√† generate\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    input_len = inputs.input_ids.shape[-1]\n",
        "    safe_max_new_tokens = 4096 - input_len\n",
        "\n",
        "    generated = model.generate(\n",
        "        input_ids = inputs.input_ids,\n",
        "        attention_mask = inputs.attention_mask,\n",
        "        max_new_tokens = safe_max_new_tokens,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "    outputs.append({\n",
        "        \"Problem\": problem,\n",
        "        \"Predicted_ProblemType\": output_text.strip()\n",
        "    })\n",
        "\n",
        "# Xu·∫•t file\n",
        "df_out = pd.DataFrame(outputs)\n",
        "df_out.to_csv(\"test_results_with_rag.csv\", index=False)\n",
        "print(\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o test_results_with_rag.csv\")\n"
      ],
      "metadata": {
        "id": "eR1IDgU7MVSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajEGrsX3j9XE"
      },
      "source": [
        "# Problem Solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBfQugWTj9XE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c026e1a3-402c-4e4e-f3d2-b27bab7d2c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 10240 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-Math-7B-bnb-4bit\", # \"unsloth/Meta-Llama-3.1-8B\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    device_map = \"auto\",\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGiayWTSj9XF"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = False,  # ‚Üê T·∫ÆT ·ªü ƒë√¢y\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9QsCrSvj9XF"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"B·∫°n l√† tr·ª£ l√Ω to√°n h·ªçc. H√£y gi·∫£i b√†i to√°n d∆∞·ªõi ƒë√¢y v·ªõi l·ªùi gi·∫£i chi ti·∫øt, t·ª´ng b∆∞·ªõc v√† l·∫≠p lu·∫≠n to√°n h·ªçc ch·∫∑t ch·∫Ω.\n",
        "\n",
        "N·∫øu b√†i to√°n y√™u c·∫ßu s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p $\\epsilon$-$\\delta$ (v√≠ d·ª•: ch·ª©ng minh gi·ªõi h·∫°n, t√≠nh li√™n t·ª•c), h√£y ƒë·∫£m b·∫£o b·∫°n √°p d·ª•ng ƒë√∫ng ph∆∞∆°ng ph√°p.\n",
        "\n",
        "S·ª≠ d·ª•ng k√Ω hi·ªáu to√°n h·ªçc ch√≠nh x√°c. B·∫°n c√≥ th·ªÉ tham kh·∫£o c√°c v√≠ d·ª• sau, **nh∆∞ng KH√îNG ƒë∆∞·ª£c l·∫∑p l·∫°i n·ªôi dung ƒë√≥.**\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "{}\n",
        "\n",
        "### Retrieved_Examples:\n",
        "{}\n",
        "\n",
        "### --- B√†i to√°n c·∫ßn gi·∫£i l√† d∆∞·ªõi ƒë√¢y ---\n",
        "\n",
        "### Solution:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # ƒê·∫£m b·∫£o th√™m EOS ƒë·ªÉ model d·ª´ng sinh\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    Problem_Type       = examples[\"problem_type\"]\n",
        "    Problem            = examples[\"problem\"]\n",
        "    Retrieved_Examples  = examples[\"Related_Knowledge\"]\n",
        "    Solution           = examples[\"solution\"]\n",
        "    texts = []\n",
        "    for problem_type, problem, related_examples, solution in zip(Problem_Type, Problem, Retrieved_Examples, Solution):\n",
        "        text = alpaca_prompt.format(problem, problem_type, related_examples, solution) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\": texts }\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('csv', data_files='/content/train_with_knowledge.csv', split='train')\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F95qkpcDj9XG"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw7v5IZNj9XG"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset, # dataset -> tokenized_train_dataset\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 2,\n",
        "\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 300, # 60 -> 10\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzUwFKSbj9XH"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7yod4Axj9XJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSstATwsj9XL"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"MathAnalysis_Qwen_ProblemSolver\") # Local saving\n",
        "tokenizer.save_pretrained(\"MathAnalysis_Qwen_ProblemSolver\")\n",
        "model.save_pretrained(\"/content/drive/MyDrive/MathAnalysis_Qwen_ProblemSolver\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/MathAnalysis_Qwen_ProblemSolver\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load corpus v√† FAISS index\n",
        "with open(\"rag_texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texts = json.load(f)\n",
        "\n",
        "index = faiss.read_index(\"rag_index.faiss\")\n",
        "embedder = SentenceTransformer('intfloat/multilingual-e5-base')\n"
      ],
      "metadata": {
        "id": "XXpaf23cC1_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(problem, k=3):\n",
        "    query = \"query: \" + problem\n",
        "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(query_embedding, k)\n",
        "    retrieved = [texts[i] for i in I[0]]\n",
        "    return \"\\n\\n---\\n\\n\".join([f\"### V√≠ d·ª• {i+1}:\\n{r}\" for i, r in enumerate(retrieved)])\n"
      ],
      "metadata": {
        "id": "KNTXRlU2C3PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"B·∫°n l√† tr·ª£ l√Ω to√°n h·ªçc. D∆∞·ªõi ƒë√¢y l√† b√†i to√°n c·∫ßn gi·∫£i. H√£y ƒë∆∞a ra l·ªùi gi·∫£i chi ti·∫øt, t·ª´ng b∆∞·ªõc, s·ª≠ d·ª•ng l·∫≠p lu·∫≠n to√°n h·ªçc ch·∫∑t ch·∫Ω.\n",
        "\n",
        "N·∫øu b√†i to√°n y√™u c·∫ßu s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p $\\epsilon$-$\\delta$ (v√≠ d·ª•: ch·ª©ng minh gi·ªõi h·∫°n, t√≠nh li√™n t·ª•c), h√£y ƒë·∫£m b·∫£o b·∫°n √°p d·ª•ng ƒë√∫ng ph∆∞∆°ng ph√°p.\n",
        "\n",
        "S·ª≠ d·ª•ng ng√¥n ng·ªØ v√† k√Ω hi·ªáu to√°n h·ªçc ch√≠nh x√°c trong su·ªët b√†i gi·∫£i.\n",
        "\n",
        "B·∫°n c√≥ th·ªÉ tham kh·∫£o c√°c v√≠ d·ª• t∆∞∆°ng t·ª± d∆∞·ªõi ƒë√¢y, **nh∆∞ng KH√îNG c·∫ßn m√¥ t·∫£ l·∫°i ho·∫∑c l·∫∑p l·∫°i c√°c v√≠ d·ª• ƒë√≥**. H√£y t·∫≠p trung gi·∫£i b√†i to√°n m·ªõi.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "{}\n",
        "\n",
        "### Retrieved_Examples:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "{}\"\"\"\n"
      ],
      "metadata": {
        "id": "mvP1smlSfh78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Load model\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"/content/drive/MyDrive/MathAnalysis_Qwen_ProblemSolver\",\n",
        "#     max_seq_length = 8192,  # v√≠ d·ª•\n",
        "#     dtype = \"float16\",\n",
        "#     load_in_4bit = True,\n",
        "# )\n",
        "# FastLanguageModel.for_inference(model)\n",
        "# model.eval()\n",
        "\n",
        "# Nh·∫≠p problem v√† problem_type\n",
        "problem = \"T√¨m t·∫•t c·∫£ c√°c gi√° tr·ªã th·ª±c c·ªßa \\( a \\) v√† \\( b \\) sao cho: \\[ (a^2 + 1)(b^2 + 1) = (a + 1)(b + 1)(ab + 1) \\]\"\n",
        "problem_type = \"Gi·∫£i ph∆∞∆°ng tr√¨nh\"\n",
        "\n",
        "# Truy h·ªìi v√≠ d·ª• t∆∞∆°ng t·ª±\n",
        "retrieved_context = retrieve_context(problem, k=3)\n",
        "# print(\"=== Retrieved Documents ===\")\n",
        "# print(retrieved_context)\n",
        "\n",
        "# T·∫°o prompt\n",
        "final_prompt = problem_prompt.format(problem, problem_type, retrieved_context)\n"
      ],
      "metadata": {
        "id": "02t60xFCfkPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T√≠nh ƒë·ªô d√†i ƒë·∫ßu v√†o an to√†n\n",
        "# input_len = inputs.input_ids.shape[-1]\n",
        "# max_total_length = max_seq_length  # v√≠ d·ª•: 10240\n",
        "# safe_max_new_tokens = max_total_length - input_len\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer([final_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "input_len = inputs.input_ids.shape[-1]\n",
        "max_new_tokens = 8192 - input_len  # Gi·ªõi h·∫°n token sinh th√™m\n",
        "\n",
        "print(f\"Input tokens: {input_len}, Max new tokens: {max_new_tokens}\")\n",
        "\n",
        "# Stream k·∫øt qu·∫£\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "\n",
        "_ = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n"
      ],
      "metadata": {
        "id": "dmWRDqItfoUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "\n",
        "# === C·∫•u h√¨nh ===\n",
        "model_name = \"MathAnalysis_Qwen_ProblemSolver\"\n",
        "max_seq_length = 4096\n",
        "dtype = torch.float16\n",
        "load_in_4bit = True\n",
        "\n",
        "# === PROMPT ===\n",
        "problem_prompt = \"\"\"V·ªõi t∆∞ c√°ch l√† tr·ª£ l√Ω to√°n h·ªçc, h√£y gi·∫£i b√†i to√°n sau. ƒê∆∞a ra l·ªùi gi·∫£i chi ti·∫øt, t·ª´ng b∆∞·ªõc b·∫±ng l·∫≠p lu·∫≠n to√°n h·ªçc ch·∫∑t ch·∫Ω. N·∫øu b√†i to√°n y√™u c·∫ßu s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p $\\epsilon$-$\\delta$, h√£y √°p d·ª•ng ƒë√∫ng. S·ª≠ d·ª•ng k√Ω hi·ªáu to√°n h·ªçc ch√≠nh x√°c.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "{}\n",
        "\n",
        "### Related_Knowledge:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# === Load model ===\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = model_name,\n",
        "#     max_seq_length = max_seq_length,\n",
        "#     dtype = dtype,\n",
        "#     load_in_4bit = load_in_4bit,\n",
        "#     device_map = \"auto\",\n",
        "#     llm_int8_enable_fp32_cpu_offload = True\n",
        "# )\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# === Load file CSV ===\n",
        "df = pd.read_csv(\"test_data.csv\")\n",
        "print(f\"T·ªïng s·ªë c√¢u: {len(df)}\")\n",
        "\n",
        "# === Chu·∫©n b·ªã l∆∞u k·∫øt qu·∫£ ===\n",
        "results = []\n",
        "\n",
        "# === Duy·ªát t·ª´ng c√¢u ===\n",
        "for idx, row in df.iterrows():\n",
        "    problem = row['problem']\n",
        "    problem_type = row['problem_type']\n",
        "\n",
        "    context = retrieve_context(problem, problem_type)  # Tr√≠ch t·ª´ knowledge_base\n",
        "\n",
        "    # Gh√©p th√†nh prompt ho√†n ch·ªânh\n",
        "    prompt = problem_prompt.format(problem, problem_type, context)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    input_len = inputs.input_ids.shape[-1]\n",
        "    safe_max_new_tokens = max_seq_length - input_len\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"‚ñ∂Ô∏è B√†i {idx+1}/{len(df)} | Input tokens: {input_len} | Max new: {safe_max_new_tokens}\")\n",
        "\n",
        "    # Stream\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        streamer=text_streamer,\n",
        "        max_new_tokens=safe_max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # L·∫•y k·∫øt qu·∫£\n",
        "    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    solution = decoded[len(prompt):].strip()  # B·ªè ph·∫ßn prompt g·ªëc\n",
        "\n",
        "    results.append({\n",
        "        \"problem\": problem,\n",
        "        \"problem_type\": problem_type,\n",
        "        \"solution\": solution\n",
        "    })\n",
        "\n",
        "# === L∆∞u ra CSV ===\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"solutions_with_rag.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"\\n‚úÖ ƒê√£ l∆∞u to√†n b·ªô l·ªùi gi·∫£i v√†o file solutions.csv\")\n"
      ],
      "metadata": {
        "id": "nGq7oM7Qr2NJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}