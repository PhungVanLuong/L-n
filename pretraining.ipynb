{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Đọc file CSV gốc\n",
        "df = pd.read_csv(\"train_data.csv\")\n",
        "df[\"Related_Knowledge\"] = df[\"problem\"].apply(lambda p: retrieve_context(p, k=3))\n",
        "df.to_csv(\"train_with_knowledge.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "FF1n4mLpVZGa",
        "outputId": "fb87af4e-d634-44ec-f6f3-cf3906556934",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `XLMRobertaSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lTDDP4YscavI",
        "outputId": "0731b4c2-37e4-4ed6-fdd2-858f5005c750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "5WXSxc72kHEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ziye2chen/LLMs-for-Mathematical-Analysis.git"
      ],
      "metadata": {
        "id": "RlY1LX02kRto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install faiss-cpu sentence-transformers\n",
        "!pip install unsloth transformers trl datasets pandas matplotlib rich scipy sentencepiece"
      ],
      "metadata": {
        "id": "lA8YuRTwkS8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZOLpBveDj9Wv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fFLO6BCzj9W1"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EN8y6T-bj9W1",
        "outputId": "3c3e947e-d630-4b28-9425-ced746c92f62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch version：\n",
            "2.7.1+cu126\n",
            "CUDA Version: \n",
            "12.6\n",
            "cuDNN version is :\n",
            "90501\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"Pytorch version：\")\n",
        "print(torch.__version__)\n",
        "print(\"CUDA Version: \")\n",
        "print(torch.version.cuda)\n",
        "print(\"cuDNN version is :\")\n",
        "print(torch.backends.cudnn.version())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Đọc file train dữ liệu để dùng cho truy hồi\n",
        "df = pd.read_csv(\"/content/train_data.csv\")\n",
        "\n",
        "# Gộp problem + solution thành văn bản corpus và thêm prefix \"passage: \"\n",
        "texts = (\"passage: \" + df['problem'] + \"\\n\\nLời giải:\\n\" + df['solution']).tolist()\n",
        "\n",
        "# Lưu corpus\n",
        "with open(\"rag_texts.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(texts, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# Tạo model embedding\n",
        "embedder = SentenceTransformer('intfloat/multilingual-e5-base')\n",
        "embeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "# Tạo FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "# Lưu index\n",
        "faiss.write_index(index, \"rag_index.faiss\")\n"
      ],
      "metadata": {
        "id": "m5ccecwu3_xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaiEj8bkj9W5"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 10240 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-Math-7B-bnb-4bit\", # \"unsloth/Meta-Llama-3.1-8B\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLHuhv-Gj9W7"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPL2DK3Bj9W8"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqa2l2Vej9W9"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Là trợ lý toán học, bạn cần phân tích bài toán để tìm ra loại bài toán đó trong Phân tích Thực tế. Cung cấp Problem_Type có thể được sử dụng để giải bài toán này.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "{}\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tEZPMCvj9W-"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    Problem_Type = examples[\"problem_type\"]\n",
        "    Problem = examples[\"problem\"]\n",
        "\n",
        "    texts = []\n",
        "    for problem, problem_type in zip(Problem, Problem_Type):\n",
        "        text = alpaca_prompt.format(problem, problem_type) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('csv',data_files = '/content/train_data.csv', split='train')\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Llz9XXpj9W_"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset, # dataset -> tokenized_train_dataset\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 2,\n",
        "\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 300, # 60 -> 10\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6ktQpOrj9XA"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7rC9fQPj9XA"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1s3cZoxj9XB"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"MathAnalysis_Qwen_Classifier\") # Local saving\n",
        "tokenizer.save_pretrained(\"MathAnalysis_Qwen_Classifier\")\n",
        "model.save_pretrained(\"/content/drive/MyDrive/MathAnalysis_Qwen_Classifier\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/MathAnalysis_Qwen_Classifier\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpkOb6jQj9XB"
      },
      "outputs": [],
      "source": [
        "test_prompt = \"\"\"Là trợ lý toán học, bạn cần phân tích bài toán để tìm ra loại bài toán đó trong Phân tích Thực tế. Cung cấp Problem_Type có thể được sử dụng để giải bài toán này.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGwYqzL4j9XC"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 10240 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"MathAnalysis_Qwen_Classifier\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# # alpaca_prompt = You MUST copy from above!\n",
        "\n",
        "# inputs = tokenizer(\n",
        "# [\n",
        "#     test_prompt.format(\n",
        "#         \"Cho ba số dương \\( a, b, c \\) thoả mãn \\( b \\ne c \\), \\( \\sqrt{a} + \\sqrt{b} \\ne \\sqrt{c} \\), \\( a + b = (\\sqrt{a} + \\sqrt{b} - \\sqrt{c})^2 \\). Chứng minh đẳng thức: \\[ \\frac{a + (\\sqrt{a} - \\sqrt{c})^2}{b + (\\sqrt{b} - \\sqrt{c})^2} = \\frac{\\sqrt{a} - \\sqrt{c}}{\\sqrt{b} - \\sqrt{c}}. \\]\", # Problem\n",
        "#     )\n",
        "# ], return_tensors = \"pt\").to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "texts = json.load(open(\"rag_texts.json\", \"r\", encoding=\"utf-8\"))\n",
        "index = faiss.read_index(\"rag_index.faiss\")\n",
        "embedder = SentenceTransformer('intfloat/multilingual-e5-base')\n",
        "\n",
        "def retrieve_context(query, k=3):\n",
        "    query_embedding = embedder.encode([\"query: \" + query])\n",
        "    D, I = index.search(query_embedding, k)\n",
        "    return [texts[i] for i in I[0]]\n",
        "\n",
        "# STEP 3: Prompt + Truy xuất + Sinh lời giải\n",
        "raw_problem = \"Cho ba số dương \\( a, b, c \\) thoả mãn \\( b \\ne c \\), \\( \\sqrt{a} + \\sqrt{b} \\ne \\sqrt{c} \\), \\( a + b = (\\sqrt{a} + \\sqrt{b} - \\sqrt{c})^2 \\). Chứng minh đẳng thức: \\[ \\frac{a + (\\sqrt{a} - \\sqrt{c})^2}{b + (\\sqrt{b} - \\sqrt{c})^2} = \\frac{\\sqrt{a} - \\sqrt{c}}{\\sqrt{b} - \\sqrt{c}}. \\]\"\n",
        "contexts = retrieve_context(raw_problem, k=3)\n",
        "context_block = \"\\n\\n\".join(contexts)\n",
        "\n",
        "final_prompt = context_block + \"\\n\\n\" + test_prompt.format(raw_problem)\n"
      ],
      "metadata": {
        "id": "L5XtmXxv57Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5iOdfsqj9XD"
      },
      "outputs": [],
      "source": [
        "# # Tự tính max_new_tokens an toàn\n",
        "# input_len = inputs.input_ids.shape[-1]\n",
        "# max_total_length = max_seq_length  # ví dụ: 10240\n",
        "# safe_max_new_tokens = max_total_length - input_len\n",
        "\n",
        "# Tokenize & sinh\n",
        "inputs = tokenizer([final_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "input_len = inputs.input_ids.shape[-1]\n",
        "safe_max_new_tokens = max_seq_length - input_len\n",
        "\n",
        "# In thử kiểm tra\n",
        "print(f\"Input tokens: {input_len}, Max new tokens: {safe_max_new_tokens}\")\n",
        "\n",
        "# Dùng TextStreamer để in ra từng dòng sinh token\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "# Sinh kết quả\n",
        "_ = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=safe_max_new_tokens,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import numpy as np\n",
        "\n",
        "test_prompt_rag = \"\"\"Bạn là trợ lý toán học. Dưới đây là một bài toán học cần phân loại Problem_Type.\n",
        "Bạn có thể sử dụng các ví dụ tương tự từ dữ liệu dưới đây để hỗ trợ.\n",
        "\n",
        "### Các ví dụ tương tự:\n",
        "{}\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Load mô hình đã fine-tune\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#         model_name = \"/content/drive/MyDrive/MathAnalysis_Qwen_Classifier\",\n",
        "#         max_seq_length = 4096,\n",
        "#         dtype = torch.float16,\n",
        "#         load_in_4bit = True,\n",
        "# )\n",
        "# FastLanguageModel.for_inference(model)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Load lại FAISS index và embedding model\n",
        "kb_df = pd.read_csv(\"train_data.csv\")\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "faiss_index = faiss.read_index(\"rag_index.faiss\")\n",
        "\n",
        "df = pd.read_csv(\"test_data.csv\")\n",
        "\n",
        "outputs = []\n",
        "for idx, row in df.iterrows():\n",
        "    problem = row[\"problem\"]\n",
        "\n",
        "    # Embed problem\n",
        "    problem_emb = embedding_model.encode([problem])\n",
        "    top_k = 3\n",
        "    distances, indices = faiss_index.search(np.array(problem_emb), top_k)\n",
        "\n",
        "    # Trích xuất context\n",
        "    retrieved_examples = []\n",
        "    for i in indices[0]:\n",
        "        example = f\"- Problem: {kb_df.iloc[i]['problem']}\\n  Problem_Type: {kb_df.iloc[i]['problem_type']}\"\n",
        "        retrieved_examples.append(example)\n",
        "    rag_context = \"\\n\".join(retrieved_examples)\n",
        "\n",
        "    # Format prompt với context\n",
        "    prompt = test_prompt_rag.format(rag_context, problem)\n",
        "\n",
        "    # Tokenize và generate\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "    input_len = inputs.input_ids.shape[-1]\n",
        "    safe_max_new_tokens = 4096 - input_len\n",
        "\n",
        "    generated = model.generate(\n",
        "        input_ids = inputs.input_ids,\n",
        "        attention_mask = inputs.attention_mask,\n",
        "        max_new_tokens = safe_max_new_tokens,\n",
        "        pad_token_id = tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    output_text = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
        "    outputs.append({\n",
        "        \"Problem\": problem,\n",
        "        \"Predicted_ProblemType\": output_text.strip()\n",
        "    })\n",
        "\n",
        "# Xuất file\n",
        "df_out = pd.DataFrame(outputs)\n",
        "df_out.to_csv(\"test_results_with_rag.csv\", index=False)\n",
        "print(\"✅ Đã lưu kết quả vào test_results_with_rag.csv\")\n"
      ],
      "metadata": {
        "id": "eR1IDgU7MVSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajEGrsX3j9XE"
      },
      "source": [
        "# Problem Solver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBfQugWTj9XE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c026e1a3-402c-4e4e-f3d2-b27bab7d2c8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 10240 # Choose any! We auto support RoPE Scaling internally!\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
        "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
        "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
        "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",          # Phi-3 2x faster!d\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
        "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen2.5-Math-7B-bnb-4bit\", # \"unsloth/Meta-Llama-3.1-8B\"\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    device_map = \"auto\",\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGiayWTSj9XF"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = False,  # ← TẮT ở đây\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9QsCrSvj9XF"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"Bạn là trợ lý toán học. Hãy giải bài toán dưới đây với lời giải chi tiết, từng bước và lập luận toán học chặt chẽ.\n",
        "\n",
        "Nếu bài toán yêu cầu sử dụng phương pháp $\\epsilon$-$\\delta$ (ví dụ: chứng minh giới hạn, tính liên tục), hãy đảm bảo bạn áp dụng đúng phương pháp.\n",
        "\n",
        "Sử dụng ký hiệu toán học chính xác. Bạn có thể tham khảo các ví dụ sau, **nhưng KHÔNG được lặp lại nội dung đó.**\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "{}\n",
        "\n",
        "### Retrieved_Examples:\n",
        "{}\n",
        "\n",
        "### --- Bài toán cần giải là dưới đây ---\n",
        "\n",
        "### Solution:\n",
        "{}\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Đảm bảo thêm EOS để model dừng sinh\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    Problem_Type       = examples[\"problem_type\"]\n",
        "    Problem            = examples[\"problem\"]\n",
        "    Retrieved_Examples  = examples[\"Related_Knowledge\"]\n",
        "    Solution           = examples[\"solution\"]\n",
        "    texts = []\n",
        "    for problem_type, problem, related_examples, solution in zip(Problem_Type, Problem, Retrieved_Examples, Solution):\n",
        "        text = alpaca_prompt.format(problem, problem_type, related_examples, solution) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\": texts }\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('csv', data_files='/content/train_with_knowledge.csv', split='train')\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F95qkpcDj9XG"
      },
      "outputs": [],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gw7v5IZNj9XG"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset, # dataset -> tokenized_train_dataset\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 2,\n",
        "\n",
        "        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 300, # 60 -> 10\n",
        "\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzUwFKSbj9XH"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7yod4Axj9XJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSstATwsj9XL"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"MathAnalysis_Qwen_ProblemSolver\") # Local saving\n",
        "tokenizer.save_pretrained(\"MathAnalysis_Qwen_ProblemSolver\")\n",
        "model.save_pretrained(\"/content/drive/MyDrive/MathAnalysis_Qwen_ProblemSolver\") # Local saving\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/MathAnalysis_Qwen_ProblemSolver\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import json\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load corpus và FAISS index\n",
        "with open(\"rag_texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    texts = json.load(f)\n",
        "\n",
        "index = faiss.read_index(\"rag_index.faiss\")\n",
        "embedder = SentenceTransformer('intfloat/multilingual-e5-base')\n"
      ],
      "metadata": {
        "id": "XXpaf23cC1_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_context(problem, k=3):\n",
        "    query = \"query: \" + problem\n",
        "    query_embedding = embedder.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(query_embedding, k)\n",
        "    retrieved = [texts[i] for i in I[0]]\n",
        "    return \"\\n\\n---\\n\\n\".join([f\"### Ví dụ {i+1}:\\n{r}\" for i, r in enumerate(retrieved)])\n"
      ],
      "metadata": {
        "id": "KNTXRlU2C3PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Bạn là trợ lý toán học. Dưới đây là bài toán cần giải. Hãy đưa ra lời giải chi tiết, từng bước, sử dụng lập luận toán học chặt chẽ.\n",
        "\n",
        "Nếu bài toán yêu cầu sử dụng phương pháp $\\epsilon$-$\\delta$ (ví dụ: chứng minh giới hạn, tính liên tục), hãy đảm bảo bạn áp dụng đúng phương pháp.\n",
        "\n",
        "Sử dụng ngôn ngữ và ký hiệu toán học chính xác trong suốt bài giải.\n",
        "\n",
        "Bạn có thể tham khảo các ví dụ tương tự dưới đây, **nhưng KHÔNG cần mô tả lại hoặc lặp lại các ví dụ đó**. Hãy tập trung giải bài toán mới.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "{}\n",
        "\n",
        "### Retrieved_Examples:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "{}\"\"\"\n"
      ],
      "metadata": {
        "id": "mvP1smlSfh78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "\n",
        "# Load model\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = \"/content/drive/MyDrive/MathAnalysis_Qwen_ProblemSolver\",\n",
        "#     max_seq_length = 8192,  # ví dụ\n",
        "#     dtype = \"float16\",\n",
        "#     load_in_4bit = True,\n",
        "# )\n",
        "# FastLanguageModel.for_inference(model)\n",
        "# model.eval()\n",
        "\n",
        "# Nhập problem và problem_type\n",
        "problem = \"Tìm tất cả các giá trị thực của \\( a \\) và \\( b \\) sao cho: \\[ (a^2 + 1)(b^2 + 1) = (a + 1)(b + 1)(ab + 1) \\]\"\n",
        "problem_type = \"Giải phương trình\"\n",
        "\n",
        "# Truy hồi ví dụ tương tự\n",
        "retrieved_context = retrieve_context(problem, k=3)\n",
        "# print(\"=== Retrieved Documents ===\")\n",
        "# print(retrieved_context)\n",
        "\n",
        "# Tạo prompt\n",
        "final_prompt = problem_prompt.format(problem, problem_type, retrieved_context)\n"
      ],
      "metadata": {
        "id": "02t60xFCfkPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tính độ dài đầu vào an toàn\n",
        "# input_len = inputs.input_ids.shape[-1]\n",
        "# max_total_length = max_seq_length  # ví dụ: 10240\n",
        "# safe_max_new_tokens = max_total_length - input_len\n",
        "\n",
        "# Tokenize\n",
        "inputs = tokenizer([final_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "input_len = inputs.input_ids.shape[-1]\n",
        "max_new_tokens = 8192 - input_len  # Giới hạn token sinh thêm\n",
        "\n",
        "print(f\"Input tokens: {input_len}, Max new tokens: {max_new_tokens}\")\n",
        "\n",
        "# Stream kết quả\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "\n",
        "_ = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    streamer=streamer,\n",
        "    max_new_tokens=max_new_tokens,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n"
      ],
      "metadata": {
        "id": "dmWRDqItfoUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "\n",
        "# === Cấu hình ===\n",
        "model_name = \"MathAnalysis_Qwen_ProblemSolver\"\n",
        "max_seq_length = 4096\n",
        "dtype = torch.float16\n",
        "load_in_4bit = True\n",
        "\n",
        "# === PROMPT ===\n",
        "problem_prompt = \"\"\"Với tư cách là trợ lý toán học, hãy giải bài toán sau. Đưa ra lời giải chi tiết, từng bước bằng lập luận toán học chặt chẽ. Nếu bài toán yêu cầu sử dụng phương pháp $\\epsilon$-$\\delta$, hãy áp dụng đúng. Sử dụng ký hiệu toán học chính xác.\n",
        "\n",
        "### Problem:\n",
        "{}\n",
        "\n",
        "### Problem_Type:\n",
        "{}\n",
        "\n",
        "### Related_Knowledge:\n",
        "{}\n",
        "\n",
        "### Solution:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# === Load model ===\n",
        "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#     model_name = model_name,\n",
        "#     max_seq_length = max_seq_length,\n",
        "#     dtype = dtype,\n",
        "#     load_in_4bit = load_in_4bit,\n",
        "#     device_map = \"auto\",\n",
        "#     llm_int8_enable_fp32_cpu_offload = True\n",
        "# )\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# === Load file CSV ===\n",
        "df = pd.read_csv(\"test_data.csv\")\n",
        "print(f\"Tổng số câu: {len(df)}\")\n",
        "\n",
        "# === Chuẩn bị lưu kết quả ===\n",
        "results = []\n",
        "\n",
        "# === Duyệt từng câu ===\n",
        "for idx, row in df.iterrows():\n",
        "    problem = row['problem']\n",
        "    problem_type = row['problem_type']\n",
        "\n",
        "    context = retrieve_context(problem, problem_type)  # Trích từ knowledge_base\n",
        "\n",
        "    # Ghép thành prompt hoàn chỉnh\n",
        "    prompt = problem_prompt.format(problem, problem_type, context)\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    input_len = inputs.input_ids.shape[-1]\n",
        "    safe_max_new_tokens = max_seq_length - input_len\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"▶️ Bài {idx+1}/{len(df)} | Input tokens: {input_len} | Max new: {safe_max_new_tokens}\")\n",
        "\n",
        "    # Stream\n",
        "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        streamer=text_streamer,\n",
        "        max_new_tokens=safe_max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Lấy kết quả\n",
        "    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    solution = decoded[len(prompt):].strip()  # Bỏ phần prompt gốc\n",
        "\n",
        "    results.append({\n",
        "        \"problem\": problem,\n",
        "        \"problem_type\": problem_type,\n",
        "        \"solution\": solution\n",
        "    })\n",
        "\n",
        "# === Lưu ra CSV ===\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv(\"solutions_with_rag.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "print(\"\\n✅ Đã lưu toàn bộ lời giải vào file solutions.csv\")\n"
      ],
      "metadata": {
        "id": "nGq7oM7Qr2NJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}